---
layout: page
---
![alt-text-1](/assets/img/Coursework.png "title")

## <font color="#E34000"><b>Mathematics in Machine Learning (by Coursera)</b></font>
<p>
I took this course to acquire an understanding of the mathematical calculations and transformations that occur behind the scenes of the code for a machine learning algorithm. The course had three parts where I learned the following:
</p>
<p><b>Linear Algebra:</b></p>
<ul>
   <li>Fundamentals of <b>vectors</b>: changing basis and linear independence.</li>
   <li><b>Matrix</b> transformations: inversion and rotation, to name a few.</li>
   <li>Einstein Summation Convention</li>
   <li><b>Gram Schmidt Process</b> with coding assignment</li>
   <li><b>Eigenvectors</b> and <b>PageRank</b> algorithm with coding assignment</li>
</ul> 

<p><b>Multivariate Calculus:</b></p>   
<ul>
   <li><b>Derivatives</b>: Jacobian, Hessian, Maclaurin Series, Taylor Series</li>
   <li>How <b>back propagation</b> relates to neural networks with coding assignment</li>
   <li>Newton-Raphson Method</li>
   <li><b>Gradient Descent</b> with coding assignment</li>
   <li>Mathematics behind <b>Simple Linear Regression</b></li>
</ul>

<b>Principal Component Analysis (PCA):</b>
<ul>
   <li><b>Statistics</b> of datasets</li>
   <li>Inner products</li>
   <li>Orthogonal <b>projections</b></li>
   <li><b>PCA:</b> the most commonly used method to reduce the dimensions of a dataset.</li>
</ul>   
