---
layout: post
title: Project code well underway
subtitle: Progress Report 5
cover-img: /assets/img/bachumushroom.jpg
thumbnail-img: /assets/img/bachumushroom.jpg
share-img: /assets/img/bachumushroom.jpg
#gh-repo: sonikarichamodur/sonikarichamodur.github.io
#gh-badge: [star, follow]
tags: [code]
comments: true
---
Importing libraries
- numpy should be imported in order to work with arrays
- matplotlib.pyplot is used to plot charts
- pandas is a library used to input the dataset, create the matrix of features and create the dependent variable vector
```python
{
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
}
```
Import dataset
read_csv() is a function from pandas that should be used to import the dataset.
```python
{
    df = pd.read_csv('mushrooms.csv')
}
```
Encoding categorical data
The entire dataset is categorical, so each column should be fitted and transformed with LabelEncoder() to convert the categorical values into numerical values.
```python
{
    from sklearn.preprocessing import LabelEncoder
    labelencoder = LabelEncoder()
    for column in df.columns:
        df[column]=labelencoder.fit_transform(df[column])
}
```
Assign matrix of features and dependent variable vector
- The matrix of features contains the independent variables
- The dependent variable vector contains labels for each instance
```python
{
    x = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values
}
```
Handle missing values
Imputation of the mean was used to fill in the missing values of the stalk-root feature with the mean of the stalk-root column. The mean was used because the values in the stalk root column do not sequentially depend on being lesser or greater than each other. 
```python
{
   from sklearn.impute import SimpleImputer
   imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
   imputer.fit(x[:, 10:11])
   x[:, 10:11] = imputer.transform(x[:, 10:11])
}
```
Split data into training set and test set
- The matrix of features is split into a training and test set
- The dependent variable vector is split into a training and test set
- The <b>train_test_split()</b> function splits the dataset into four matrices
```python
{
    from sklearn.model_selection import train_test_split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)
}
```
Feature scaling
- Standardization was tried first, still yet to make a more mindful choice about feature scaling technique
```python
{
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    x_train[:, :] = sc.fit_transform(x_train[:, :])
    x_test[:, :] = sc.transform(x_test[:, :])
}
```
Logistic regression
- The first binary classifier was trained using logistic regression, where the dataset was fit on the training set for the matrix of features and dependent variable vector, and the vector of predictions was made by running the trained classifier on the test set of the matrix of features.
```python
{
    from sklearn.linear_model import LogisticRegression
    classifier = LogisticRegression(random_state = 0)
    classifier.fit(x_train,y_train)
    y_pred = classifier.predict(x_test)
    print("Test Accuracy: {}%".format(round(lr.score(x_test,y_test)*100,2)))
}
```
<b>Test Accuracy:</b> 96.0%

The logistic regression model obtained an accuracy of 96.0% for this dataset. 
